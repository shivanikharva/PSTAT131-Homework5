---
title: "PSTAT131 Homework 5"
author: "Shivani Kharva"
date: "2022-11-07"
output:
  html_document:
    toc: true
---

### Initial Setup  

```{r, message = FALSE}
# Loading the data/ packages
pokemon_data <- read.csv("data/Pokemon.csv")
library(tidymodels)
library(ISLR)
library(tidyverse)
library(discrim)
library(poissonreg)
library(glmnet)
tidymodels_prefer()
set.seed(0124)
```

### Exercise 1  
```{r}
# Loading in the `janitor` package
library(janitor)

# Using clean_names() on the pokemon data
pokemon_clean <- clean_names(pokemon_data)

# For comparison ...
head(pokemon_data)
head(pokemon_clean)
```

It appears that clean_names() changed the column names to a cleaner format without any extra characters or symbols and all in lower-case font. I think clean_names() is useful because it cleans up the variable names and simplifies them. If variable names are complicated, it can become confusing so having a function like clean_names() that automatically simplifies the variable/column names is quite useful.  

### Exercise 2  
```{r}
# Creating a bar chart of `type_1`
type_1_bar_chart <- ggplot(pokemon_clean, aes(y = type_1)) +
  geom_bar()
type_1_bar_chart
```

There are 18 classes of the outcome (18 different types for type 1 of the Pokemon).The Pokemon type that has the fewest Pokemon is the Flying type. The type with the second fewest Pokemon is Fairy.  

```{r}
# Filtering the data set to only contain Pokemon of the given types
pokemon_final <- pokemon_clean %>% 
  filter(type_1 %in% c("Bug", "Fire", "Grass", "Normal", "Water", "Psychic"))
```

```{r}
# Converting given variables to factors
pokemon_final$type_1 <- as.factor(pokemon_final$type_1)
pokemon_final$legendary <- as.factor(pokemon_final$legendary)
pokemon_final$generation <- as.factor(pokemon_final$generation)
```

### Exercise 3  

```{r}
# Splitting the data and stratifying by `type_1`
pokemon_split <- initial_split(pokemon_final, prop = 0.7, strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```

```{r}
# Verifying that the training and testing data have the correct number of outcomes
nrow(pokemon_train)/nrow(pokemon_final)
nrow(pokemon_test)/nrow(pokemon_final)
```

The training data has \~70% of the observations of the original data set and the testing data has \~30% of the observations of the original data set.   

```{r}
# Using v-fold cross validation with 5 folds and type_1 strata
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata = type_1)
```

Stratifying the folds might be a good idea in order to ensure that each fold has the same distribution of Pokemon of each type since there are not an equal number of Pokemon for each type chosen.  

### Exercise 4  

```{r}
# Setting up the recipe
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pokemon_train) %>% 
  # Dummy coding `legendary` and `generation`
  step_dummy(all_nominal_predictors()) %>% 
  # Centering and scaling all predictors
  step_normalize(all_predictors())
```

### Exercise 5  

```{r}
# Setting up the model
multinom_model <- multinom_reg(mixture = tune(), penalty = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

# Setting up the workflow
multinom_workflow <- workflow() %>% 
  # Adding the model
  add_model(multinom_model) %>% 
  # Adding the recipe
  add_recipe(pokemon_recipe)
```

```{r}
# Creating a grid for penalty and mixture
penalty_mixture_grid <- grid_regular(penalty(range = c(-5, 5)), mixture(range = c(0,1)), levels = 10)
penalty_mixture_grid
```

We will be fitting 500 (100 combinations of penalty and mixture for the model * 5 folds) models.  

### Exercise 6  

```{r, eval = FALSE}
# Fitting the models
tune_res <- tune_grid(
  multinom_workflow,
  resamples = pokemon_folds,
  grid = penalty_mixture_grid
)

write_rds(tune_res, file = "tuned_elastic.rds")
```

```{r}
# Loading the fit
tune_res <- read_rds(file = "tuned_elastic.rds")

# Using autoplot() on the results
autoplot(tune_res)
```

Explanation
The largest values of penalty appear to have the lowest accuracy and and ROC AUC. As we increase the penalty, the model does terribly when it gets too high because the high penalty value causes the slopes to be shrunk so far down that the model cannot predict accurately.  

Also, from the plot, it appears that lower values of mixture have a higher accuracy (since the higher values appear to drop the most after the first peak). Also, the mid to lower values of mixture have a higher ROC AUC than the higher values and it is apparent from the graph that the ROC AUC of the higher values of mixture drops down much faster than that of the lower values. This makes sense because lower values of mixture relate to ridge regression while higher values of mixture relate to lasso regression. Ridge has a higher ROC AUC than lasso for longer because lasso shrinks coefficients earlier than it needs. From the graph, we can see that lasso regression underperforms ridge regression at higher values of lambda (penalty term).  

### Exercise 7  

```{r}
# Choosing the model with the optimal roc_auc
best_model <- select_best(tune_res, metric = "roc_auc")
best_model
```

```{r}
# Finalizing the workflow
final_workflow <- finalize_workflow(multinom_workflow, best_model)

# Fitting the model to the training set
final_fit <- fit(final_workflow, pokemon_train)

# Assessing the accuracy on the testing set
final_tibble <- augment(final_fit, new_data = pokemon_test) 

testing_acc <- final_tibble %>% 
  accuracy(truth = type_1, estimate = .pred_class)
testing_acc
```

The accuracy of the model on the testing set is 0.3428571.  

### Exercise 8  

```{r}
# Calculating the overall ROC AUC on the testing set
final_tibble %>% 
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)

# Creating plots of the different ROC
all_roc_curves <- final_tibble %>% 
  roc_curve(truth = type_1, estimate = .pred_Bug:.pred_Water) %>% 
  autoplot()
all_roc_curves
```

```{r}
# Creating and visualizing confusion matrix
confusion_matrix <- final_tibble %>%
  conf_mat(type_1, .pred_class) %>% 
  autoplot(type = "heatmap")
confusion_matrix
```

I notice that the overall ROC AUC (~0.7) is relatively good, but from the individual ROC plots, it is apparent that the model does worse for some types than others. From the ROC plots, the types the model is best at differentiating from other types appear to be Normal and Psychic (more so for Normal). The types the model does slightly worse at differentiating from other types but still relatively okay are Bug, Fire, and Grass. However, the type the model is worst at differentiating from other types (no better than a coin flip) is Water.  

From the confusion matrix, we can see that the type the model predicts the most is Water, which explains why the ROC curve for water shows such a low ROC AUC (the model is not very good at discriminating Water from the other types). The model may make the most accurate predictions for the Water type overall, but it makes many more inaccurate predictions of the Water type than accurate predictions, which where it predicts the Pokemon is of Water type when it really is not (19 accurate predictions vs. 49 inaccurate predictions). From those inaccurate predictions, we can see that the model has quite poor prediction of Grass because it inaccurately classifies it as Water more so than the other types. This makes intuitive sense because those two may have similar qualities (just based on substantive knowledge about the relationship between grass and water).   

Also, it is apparent that the model may be slightly better at predicting the Normal type Pokemon as it has more accurate predictions of the Normal type than inaccurate predictions (16 accurate predictions vs. 15 inaccurate predictions).  

The model might be worst at differentiating the Water type Pokemon because there are significantly more Water type Pokemon in our data set than any other type so it may be defaulting to Water when it cannot differentiate the POkemon into any other specific type.   










